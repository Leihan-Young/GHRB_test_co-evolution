{
  "issue_id": 6154,
  "issue_url": "https://github.com/apache/rocketmq/issues/6154",
  "title": "Support Amazon S3 backend in TieredStorage",
  "description": "<h1 dir=\"auto\">Support S3 backend for TiredStorage</h1>\n<h2 dir=\"auto\">Target</h2>\n<ol dir=\"auto\">\n<li>Implement Amazon S3 backend for TieredStorage.</li>\n<li>Optimize upload and download efficiency.</li>\n</ol>\n<h2 dir=\"auto\">Implment</h2>\n<h3 dir=\"auto\">Core Idea</h3>\n<p dir=\"auto\">When the upper layer calls <code class=\"notranslate\">commit0</code>, the data is directly uploaded, and our upload buffer and pre-read mechanism are implemented in <code class=\"notranslate\">TieredFileSegment</code> and <code class=\"notranslate\">TieredMessageFetcher</code>, respectively. We do not need to do optimization at this layer and treat commit0 as the method that is actually persisted to S3. When we fill an entire <code class=\"notranslate\">Segment</code>, we can use <code class=\"notranslate\">UploadPartCopy</code> to actually organize these objects of different sizes into one large object.</p>\n<h3 dir=\"auto\">Complete Process</h3>\n<p dir=\"auto\">We need to maintain metadata <code class=\"notranslate\">S3SegmentMetadata</code> for this <code class=\"notranslate\">Segment</code>, which maintains the mapping between the <code class=\"notranslate\">position</code> of this logical <code class=\"notranslate\">Segment</code> and the directory and size of the file on S3. For example, the internal small file metadata of the first <code class=\"notranslate\">Segment</code> of <code class=\"notranslate\">CommitLog</code> of <code class=\"notranslate\">queue1</code>.</p>\n<ul dir=\"auto\">\n<li><code class=\"notranslate\">0</code> -> <code class=\"notranslate\"><clusterName1/brokerName1/topic1/queue1/commitLog/seg-0/chunk-0, 1024></code></li>\n<li><code class=\"notranslate\">1024</code> -> <code class=\"notranslate\"><clusterName1/brokerName1/topic1/queue1/commitLog/seg-0/chunk-1024, 4096></code></li>\n<li><code class=\"notranslate\">5120</code> -> <code class=\"notranslate\"><clusterName1/brokerName1/topic1/queue1/commitLog/seg-0/chunk-5120, 5120></code></li>\n</ul>\n<blockquote>\n<p dir=\"auto\">Create the S3SegmentFile object.</p>\n</blockquote>\n<p dir=\"auto\">That is, call the <code class=\"notranslate\">S3Segment#S3Segment</code> constructor function. The path of the logical <code class=\"notranslate\">Segment</code> file is concatenated according to the rules, and the format is as follows: <code class=\"notranslate\">{clusterName}/{brokerName}/{topic}/{queue}/{type}/seg-{baseOffset}</code> is used as the path of the logical Segment file. The path below is called <code class=\"notranslate\">baseDir</code>.</p>\n<blockquote>\n<p dir=\"auto\">Build S3SegementMetadata</p>\n</blockquote>\n<p dir=\"auto\">Get the objects under <code class=\"notranslate\">baseDir</code> and construct the metadata.</p>\n<blockquote>\n<p dir=\"auto\">Commit</p>\n</blockquote>\n<p dir=\"auto\">That is, the upper layer calls the <code class=\"notranslate\">S3Segment#commit0()</code> method. Suppose to write data with <code class=\"notranslate\">position=0</code> and <code class=\"notranslate\">length=1024</code>, that is, write to the object path <code class=\"notranslate\">baseDir/chunk-0</code> with a size of <code class=\"notranslate\">1024</code>. Upload it directly through the S3 client asynchronously. After the upload is completed, update the <code class=\"notranslate\">S3SegmentMetadata</code>:</p>\n<p dir=\"auto\"><code class=\"notranslate\">0</code> -> <code class=\"notranslate\"><baseDir/chunk-0, 1024></code></p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/5c2e2da0dac084fca980b6399ca78e6a5c3570ed8e1646d58f4f809fe6419629/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339383838393033342d65643632613635642d303661662d343763372d393836662d3039376230616365663564612e6a706567\"><img src=\"https://camo.githubusercontent.com/5c2e2da0dac084fca980b6399ca78e6a5c3570ed8e1646d58f4f809fe6419629/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339383838393033342d65643632613635642d303661662d343763372d393836662d3039376230616365663564612e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1679398889034-ed62a65d-06af-47c7-986f-097b0acef5da.jpeg\" style=\"max-width: 100%;\"></a></p>\n<blockquote>\n<p dir=\"auto\">Read</p>\n</blockquote>\n<p dir=\"auto\">That is, call the <code class=\"notranslate\">S3Segment#read0()</code> method. Suppose to read 1M data with <code class=\"notranslate\">position=0</code> and <code class=\"notranslate\">length=1024</code>. Then you can find the object <code class=\"notranslate\">baseDir/chunk-0</code> starting from position 0 through <code class=\"notranslate\">S3SegmentMetadata</code>, and download it directly through the S3 client.</p>\n<blockquote>\n<p dir=\"auto\">Commit</p>\n</blockquote>\n<p dir=\"auto\">Suppose we write data with <code class=\"notranslate\">position=1024</code> and <code class=\"notranslate\">length=4096</code>, that is, submit 4K data from position 1K. Upload it directly through the S3 client asynchronously. After the upload is completed, update the <code class=\"notranslate\">S3SegmentMetadata</code>:</p>\n<p dir=\"auto\"><code class=\"notranslate\">0</code> -> <code class=\"notranslate\"><baseDir/chunk-0, 1024></code><br>\n<code class=\"notranslate\">1024</code> -> <code class=\"notranslate\"><baseDir/chunk-1024, 4096></code></p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/6f41000079f9d7a2167d33691a996a238301586257f7d9d8be8ce4463f76c7bb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339343430383134312d61363431346437332d396438322d343531362d613831362d3839643830323132663065612e6a706567\"><img src=\"https://camo.githubusercontent.com/6f41000079f9d7a2167d33691a996a238301586257f7d9d8be8ce4463f76c7bb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339343430383134312d61363431346437332d396438322d343531362d613831362d3839643830323132663065612e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1679394408141-a6414d73-9d82-4516-a816-89d80212f0ea.jpeg\" style=\"max-width: 100%;\"></a></p>\n<blockquote>\n<p dir=\"auto\">read</p>\n</blockquote>\n<p dir=\"auto\">Assuming we are reading from <code class=\"notranslate\">position = 512</code> and <code class=\"notranslate\">length = 1024</code>, according to S3SegmentMetadata, we need to fetch the data of <code class=\"notranslate\">[512, 1024)</code> from <code class=\"notranslate\">baseDir/chunk-0</code> and <code class=\"notranslate\">[1024, 1536)</code> from <code class=\"notranslate\">baseDir/chunk-1024</code>.</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/8fc21264fd59c4323558f60e0aebfcc265642510a2cbe3fdd831dff434ba4060/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339363536333039332d33333437353463392d636438652d346637652d616435362d3065616638316163346139382e6a706567\"><img src=\"https://camo.githubusercontent.com/8fc21264fd59c4323558f60e0aebfcc265642510a2cbe3fdd831dff434ba4060/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339363536333039332d33333437353463392d636438652d346637652d616435362d3065616638316163346139382e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1679396563093-334754c9-cd8e-4f7e-ad56-0eaf81ac4a98.jpeg\" style=\"max-width: 100%;\"></a></p>\n<blockquote>\n<p dir=\"auto\">commit</p>\n</blockquote>\n<p dir=\"auto\">Assuming we are writing data with <code class=\"notranslate\">position = 5120</code> and <code class=\"notranslate\">length = 5120</code>, we upload the object <code class=\"notranslate\">baseDir/chunk-5120</code>, and the <code class=\"notranslate\">S3SegmentMetadata</code> becomes:</p>\n<p dir=\"auto\"><code class=\"notranslate\">0</code> -> <code class=\"notranslate\"><baseDir/chunk-0, 1024></code><br>\n<code class=\"notranslate\">1024</code> -> <code class=\"notranslate\"><baseDir/chunk-1024, 4096></code><br>\n<code class=\"notranslate\">5120</code> -> <code class=\"notranslate\"><baseDir/chunk-5120, 5120></code></p>\n<blockquote>\n<p dir=\"auto\">Segment full</p>\n</blockquote>\n<p dir=\"auto\">Assuming our <code class=\"notranslate\">Segment</code> size is <code class=\"notranslate\">10240</code> Bytes. We have filled the above <code class=\"notranslate\">Segment</code> by now.<br>\nWe can asynchronously trigger an <code class=\"notranslate\">uploadPartCopy</code> to consolidate all the <code class=\"notranslate\">chunks</code> into a large <code class=\"notranslate\">segment</code> object. The path is <code class=\"notranslate\">baseDir/segment</code>. After the copy is successful, we can asynchronously delete all <code class=\"notranslate\">baseDir/chunk-*</code>, and update <code class=\"notranslate\">S3SegmentMetadata</code> to:</p>\n<p dir=\"auto\"><code class=\"notranslate\">0</code> -> <code class=\"notranslate\"><baseDir/segment, 10240></code></p>\n<blockquote>\n<p dir=\"auto\">Restart - Consolidated</p>\n</blockquote>\n<p dir=\"auto\">At this point, we first concatenate the path <code class=\"notranslate\">baseDir/segment</code> to determine whether the object exists. If it exists, it is a consolidated large <code class=\"notranslate\">Segment</code> object, so we record the corresponding metadata locally. <code class=\"notranslate\">read0()</code> can directly read the object based on the offset. Then, we check whether the <code class=\"notranslate\">baseDir/chunk-*</code> objects currently exist. If they do, it means that the asynchronous deletion has not been successful, so we can try to delete them again asynchronously.</p>\n<blockquote>\n<p dir=\"auto\">Restart - Not Consolidated</p>\n</blockquote>\n<p dir=\"auto\">If the concatenated path <code class=\"notranslate\">baseDir/segment</code> does not exist, it may be due to consolidation failure or the current <code class=\"notranslate\">Segment</code> not being fully written. We can list all the <code class=\"notranslate\">chunk-*</code> paths under <code class=\"notranslate\">baseDir</code> and then determine whether they are full (an interface can be added to determine this during recovery). If it is full, we can consolidate and delete it asynchronously. If it is not full, we can restore the metadata <code class=\"notranslate\">S3SegmentMetadata</code> normally.</p>\n<h3 dir=\"auto\">Possible Optimizations</h3>\n<h4 dir=\"auto\">Upload Buffer Pooling</h4>\n<p dir=\"auto\">A general <code class=\"notranslate\">UploadBufferPool</code> can be used as the upload buffer. Each time <code class=\"notranslate\">commit0</code> is called, the data is first put into the corresponding Buffer in the pool. When the overall buffer pool reaches the set threshold, the actual data upload is triggered.</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/45f9c75a9c8761d6c712c3786da4071da4769fdfddfadc860bf339fc4f7479ed/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339383236323238382d62316363633661342d663631332d346437632d626538352d3134646466303737353866612e6a706567\"><img src=\"https://camo.githubusercontent.com/45f9c75a9c8761d6c712c3786da4071da4769fdfddfadc860bf339fc4f7479ed/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637393339383236323238382d62316363633661342d663631332d346437632d626538352d3134646466303737353866612e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1679398262288-b1ccc6a4-f613-4d7c-be85-14ddf07758fa.jpeg\" style=\"max-width: 100%;\"></a></p>\n<ul dir=\"auto\">\n<li>When <code class=\"notranslate\">commit0</code> is called, the data is read into the corresponding <code class=\"notranslate\">Buffer</code> in the <code class=\"notranslate\">queue</code>.</li>\n<li>When the <code class=\"notranslate\">Upload Buffer Pool</code> reaches the threshold, the actual data is uploaded to S3. All the data in each <code class=\"notranslate\">queue</code>'s <code class=\"notranslate\">Buffer</code> forms an object.</li>\n<li>Update <code class=\"notranslate\">S3SegmentMetadata</code> and <code class=\"notranslate\">TieredFileSegment#commitPosition</code>.</li>\n</ul>\n<h3 dir=\"auto\">Tasks</h3>\n<ol dir=\"auto\">\n<li>Implement and run the basic solution.</li>\n<li>Test the performance of the basic solution.</li>\n<li>If performance is related to uploading in batches, implement an optimization solution with uploading buffering pooling and compare its performance.</li>\n</ol>\n<h2 dir=\"auto\">Elimination implement</h2>\n<h3 dir=\"auto\">new configuration</h3>\n<table role=\"table\">\n<thead>\n<tr>\n<th>Configuration</th>\n<th>Type</th>\n<th>Unit</th>\n<th>Default Value</th>\n<th>Function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>s3Region</td>\n<td>String</td>\n<td></td>\n<td></td>\n<td>Region name about S3 service</td>\n</tr>\n<tr>\n<td>s3Bucket</td>\n<td>String</td>\n<td></td>\n<td></td>\n<td>object's store bucket in S3</td>\n</tr>\n<tr>\n<td>s3AccessKey</td>\n<td>String</td>\n<td></td>\n<td></td>\n<td>IAM's accessKey</td>\n</tr>\n<tr>\n<td>s3SecretAccessKey</td>\n<td>String</td>\n<td></td>\n<td></td>\n<td>IAM's Secret AccessKey</td>\n</tr>\n<tr>\n<td>s3ChunkSize</td>\n<td>long</td>\n<td>bytes</td>\n<td><code class=\"notranslate\">4 *1024* 1024</code></td>\n<td><code class=\"notranslate\">chunk</code> num in one <code class=\"notranslate\">S3Segment</code></td>\n</tr>\n<tr>\n<td>readaheadChunkNum</td>\n<td>int</td>\n<td></td>\n<td>8</td>\n<td>readahead <code class=\"notranslate\">chunk</code> num in each <code class=\"notranslate\">read0</code> calling</td>\n</tr>\n<tr>\n<td>s3ClientThreadNum</td>\n<td>int</td>\n<td></td>\n<td><code class=\"notranslate\">tieredStoreGroupCommitSize</code>/<code class=\"notranslate\">s3ChunkSize</code></td>\n<td>threads' num in S3Client's threadpool</td>\n</tr>\n</tbody>\n</table>\n<p dir=\"auto\">A segment is treated as a logical file and is divided into multiple physical files, or multiple physical objects, in the S3 view. We assume that each physical object has a default size of 4 MB, which is named <code class=\"notranslate\">chunk</code>.</p>\n<p dir=\"auto\">For ease of process representation, we assume that readaheadChunkNum is 2 in the following.</p>\n<h3 dir=\"auto\">Process</h3>\n<blockquote>\n<p dir=\"auto\">Create <code class=\"notranslate\">S3SegmentFile</code> object\u3002</p>\n</blockquote>\n<p dir=\"auto\">This is done in the <code class=\"notranslate\">S3Segment#S3Segment</code> constructor. The path of the logical segment file is constructed by concatenating the following components according to a set of rules: <code class=\"notranslate\">clusterName/brokerName/topic/queue/type-baseOffset</code>. The path below this point is referred to as <code class=\"notranslate\">baseDir</code>.</p>\n<blockquote>\n<p dir=\"auto\">Create real logical <code class=\"notranslate\">segment</code></p>\n</blockquote>\n<p dir=\"auto\">That is, the <code class=\"notranslate\">S3Segment#createFile()</code> method is called. Since no data has been written yet, we need to create the first <code class=\"notranslate\">chunk</code> object and allocate 4MB of memory to cache the data for this chunk. We request the creation of an object from S3 in the format <code class=\"notranslate\">baseDir/chunk-startOffset</code>, which means creating a <code class=\"notranslate\">baseDir/chunk-0</code> object in S3 now.</p>\n<blockquote>\n<p dir=\"auto\">commit</p>\n</blockquote>\n<p dir=\"auto\">The <code class=\"notranslate\">Segment#commit0()</code> method is called.<br>\nWe assume that wrting 2MB data this time.</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/e41696830f8dbff1f8591f3a5b9f209c9dc630f7e6073c0736920c94d7ccbdac/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233333235353337332d31636165373030322d326530642d343737612d623735392d3035326432383662393434312e6a706567\"><img src=\"https://camo.githubusercontent.com/e41696830f8dbff1f8591f3a5b9f209c9dc630f7e6073c0736920c94d7ccbdac/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233333235353337332d31636165373030322d326530642d343737612d623735392d3035326432383662393434312e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1677233255373-1cae7002-2e0d-477a-b759-052d286b9441.jpeg\" style=\"max-width: 100%;\"></a></p>\n<p dir=\"auto\">The data is directlly writed into <code class=\"notranslate\">chunk-0</code>, and uploaded to S3.</p>\n<blockquote>\n<p dir=\"auto\">read</p>\n</blockquote>\n<p dir=\"auto\">That is, the <code class=\"notranslate\">S3Segment#read0()</code> method is called. Suppose we are currently reading 1MB of data with <code class=\"notranslate\">position = 0 and length = 1024</code>. Then it directly hits in the local <code class=\"notranslate\">chunk-0</code> buffer and returns.</p>\n<blockquote>\n<p dir=\"auto\">commit</p>\n</blockquote>\n<p dir=\"auto\">Suppose this time we write <code class=\"notranslate\">position= 2048, length= 12 * 1024</code> data, that is, submit 12MB of data from 2MB position.</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/b19f402b0e088a830db20580eea4e7978a15ba931a7274b00ae14edaece5ee92/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233333930383534302d61633962386565632d336362302d346539362d386565612d6438316635313832616239322e6a706567\"><img src=\"https://camo.githubusercontent.com/b19f402b0e088a830db20580eea4e7978a15ba931a7274b00ae14edaece5ee92/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233333930383534302d61633962386565632d336362302d346539362d386565612d6438316635313832616239322e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1677233908540-ac9b8eec-3cb0-4e96-8eea-d81f5182ab92.jpeg\" style=\"max-width: 100%;\"></a></p>\n<p dir=\"auto\">At this point, the first 2MB of chunk-0 is cached locally, so we can directly concatenate the first 2MB of <code class=\"notranslate\">chunk-0</code> with the first 2MB of the stream to form a complete <code class=\"notranslate\">chunk-0</code>. Next, we correctly locate the first 2MB of <code class=\"notranslate\">chunk-4096</code>, <code class=\"notranslate\">chunk-8192</code>, and <code class=\"notranslate\">chunk-12288</code>, and then upload them to S3. For the case of multiple chunks uploading at the same time, we use asynchronous/thread pool to upload them. If some chunks fail to upload, they are cached and then retried in the background asynchronously. If they fail multiple times, appropriate logical processing is performed.</p>\n<blockquote>\n<p dir=\"auto\">read</p>\n</blockquote>\n<p dir=\"auto\">After the above commit, only the first 2MB of <code class=\"notranslate\">chunk-12288</code> is cached locally. Now, we read 4096 bytes of data starting from <code class=\"notranslate\">position = 2048</code>, which means reading the second half of <code class=\"notranslate\">chunk-0</code> and the first half of <code class=\"notranslate\">chunk-4096</code>. Since we have enabled the pre-reading mechanism and the parameter is 2, we need to read two more chunks. Considering that we only read half of <code class=\"notranslate\">chunk-4096</code>, we only need to read one more chunk, which is <code class=\"notranslate\">chunk-8192</code>.</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/6c9050a6e7d59ec0b67f7daffc63630985bd9f6ce5aef97707a3683b8aa3396d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233343430373036352d61383936356663372d663364392d346433322d623538332d3466393865613438356466342e6a706567\"><img src=\"https://camo.githubusercontent.com/6c9050a6e7d59ec0b67f7daffc63630985bd9f6ce5aef97707a3683b8aa3396d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233343430373036352d61383936356663372d663364392d346433322d623538332d3466393865613438356466342e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1677234407065-a8965fc7-f3d9-4d32-b583-4f98ea485df4.jpeg\" style=\"max-width: 100%;\"></a></p>\n<p dir=\"auto\">Then we read <code class=\"notranslate\">chunk-0</code>, <code class=\"notranslate\">chunk-4096</code>, and <code class=\"notranslate\">chunk-8192</code> from S3. According to the pre-reading mechanism, we do not save <code class=\"notranslate\">chunk-0</code> and only save <code class=\"notranslate\">chunk-4096</code> and <code class=\"notranslate\">chunk-8192</code> in memory.</p>\n<p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://camo.githubusercontent.com/0ba4c6afb73c550d2662a02994d7fd21dd2d7494b45fb02a67a2fe42f1daa467/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233343734333236332d33323561386632612d323935382d343563622d393132642d3038383433623832376662372e6a706567\"><img src=\"https://camo.githubusercontent.com/0ba4c6afb73c550d2662a02994d7fd21dd2d7494b45fb02a67a2fe42f1daa467/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032332f6a7065672f32323434363935362f313637373233343734333236332d33323561386632612d323935382d343563622d393132642d3038383433623832376662372e6a706567\" alt=\"\" data-canonical-src=\"https://cdn.nlark.com/yuque/0/2023/jpeg/22446956/1677234743263-325a8f2a-2958-45cb-912d-08843b827fb7.jpeg\" style=\"max-width: 100%;\"></a></p>\n<blockquote>\n<p dir=\"auto\">read</p>\n</blockquote>\n<p dir=\"auto\">Now, we read 4096 bytes of data starting from <code class=\"notranslate\">position = 6144</code>, which means reading the second half of <code class=\"notranslate\">chunk-4096</code> and the first half of <code class=\"notranslate\">chunk-8192</code>. Since we have pre-loaded <code class=\"notranslate\">chunk-4096</code> and <code class=\"notranslate\">chunk-8192</code> into memory, we can directly return the data without reading from S3.</p>\n<blockquote>\n<p dir=\"auto\"><code class=\"notranslate\">Segment</code> is full</p>\n</blockquote>\n<p dir=\"auto\">At this point, we can asynchronously trigger an <code class=\"notranslate\">uploadPartCopy</code> operation to consolidate all the <code class=\"notranslate\">chunks</code> into a single large <code class=\"notranslate\">segment</code> object, and record the basic information of the <code class=\"notranslate\">segment</code> in the object metadata. The object path is <code class=\"notranslate\">clusterName/brokerName/topic/queue/type-baseOffset-seg</code>. After the copy operation is successful, we can asynchronously delete the parent path of the chunks.</p>\n<blockquote>\n<p dir=\"auto\">restart with <code class=\"notranslate\">segment</code></p>\n</blockquote>\n<p dir=\"auto\">Now we concatenate the path <code class=\"notranslate\">clusterName/brokerName/topic/queue/type-baseOffset-seg</code> and check whether the object exists. If it exists, it means it is the already organized large <code class=\"notranslate\">Segment</code> object, then we record the corresponding metadata locally, and <code class=\"notranslate\">read0()</code> can directly read the object based on the offset. Next, we check if there is an object under <code class=\"notranslate\">.../type-baseOffset</code>. If it exists, it means the asynchronous deletion has not been successful, so we can re-attempt asynchronous deletion.</p>\n<blockquote>\n<p dir=\"auto\">restart with unexist <code class=\"notranslate\">segment</code></p>\n</blockquote>\n<p dir=\"auto\">If the path <code class=\"notranslate\">.../type-baseOffset-seg</code> does not exist, it may be due to failed consolidation or the current <code class=\"notranslate\">segment</code> has not been written to capacity. In this case, we can list all the chunk files under the path and then determine if the segment has been fully written (this can be checked by adding an interface that is called during recovery). If the <code class=\"notranslate\">segment</code> has been fully written, we can consolidate the <code class=\"notranslate\">chunks</code> asynchronously and then delete them. If the <code class=\"notranslate\">segment</code> has not been fully written, we can simply recover the latest <code class=\"notranslate\">chunk</code> by caching it.</p>\n<h3 dir=\"auto\">Advantages and disadvantages</h3>\n<blockquote>\n<p dir=\"auto\">Advantages</p>\n</blockquote>\n<ol dir=\"auto\">\n<li>The pre-fetching mechanism and the caching of incomplete chunks can help reduce network requests.</li>\n<li>Optimal use case for this design is sequential read access, which fully utilizes the prefetching mechanism.</li>\n</ol>\n<blockquote>\n<p dir=\"auto\">Disadvantages</p>\n</blockquote>\n<ol dir=\"auto\">\n<li><code class=\"notranslate\">chunk</code> caches can lead to excessive memory usage. Suppose that 1000 queues, even if only one <code class=\"notranslate\">chunk</code> is cached for one queue, can reach 4GB of memory usage.</li>\n</ol>",
  "description_text": "Support S3 backend for TiredStorage\nTarget\n\nImplement Amazon S3 backend for TieredStorage.\nOptimize upload and download efficiency.\n\nImplment\nCore Idea\nWhen the upper layer calls commit0, the data is directly uploaded, and our upload buffer and pre-read mechanism are implemented in TieredFileSegment and TieredMessageFetcher, respectively. We do not need to do optimization at this layer and treat commit0 as the method that is actually persisted to S3. When we fill an entire Segment, we can use UploadPartCopy to actually organize these objects of different sizes into one large object.\nComplete Process\nWe need to maintain metadata S3SegmentMetadata for this Segment, which maintains the mapping between the position of this logical Segment and the directory and size of the file on S3. For example, the internal small file metadata of the first Segment of CommitLog of queue1.\n\n0 -> \n1024 -> \n5120 -> \n\n\nCreate the S3SegmentFile object.\n\nThat is, call the S3Segment#S3Segment constructor function. The path of the logical Segment file is concatenated according to the rules, and the format is as follows: {clusterName}/{brokerName}/{topic}/{queue}/{type}/seg-{baseOffset} is used as the path of the logical Segment file. The path below is called baseDir.\n\nBuild S3SegementMetadata\n\nGet the objects under baseDir and construct the metadata.\n\nCommit\n\nThat is, the upper layer calls the S3Segment#commit0() method. Suppose to write data with position=0 and length=1024, that is, write to the object path baseDir/chunk-0 with a size of 1024. Upload it directly through the S3 client asynchronously. After the upload is completed, update the S3SegmentMetadata:\n0 -> \n\n\nRead\n\nThat is, call the S3Segment#read0() method. Suppose to read 1M data with position=0 and length=1024. Then you can find the object baseDir/chunk-0 starting from position 0 through S3SegmentMetadata, and download it directly through the S3 client.\n\nCommit\n\nSuppose we write data with position=1024 and length=4096, that is, submit 4K data from position 1K. Upload it directly through the S3 client asynchronously. After the upload is completed, update the S3SegmentMetadata:\n0 -> \n1024 -> \n\n\nread\n\nAssuming we are reading from position = 512 and length = 1024, according to S3SegmentMetadata, we need to fetch the data of [512, 1024) from baseDir/chunk-0 and [1024, 1536) from baseDir/chunk-1024.\n\n\ncommit\n\nAssuming we are writing data with position = 5120 and length = 5120, we upload the object baseDir/chunk-5120, and the S3SegmentMetadata becomes:\n0 -> \n1024 -> \n5120 -> \n\nSegment full\n\nAssuming our Segment size is 10240 Bytes. We have filled the above Segment by now.\nWe can asynchronously trigger an uploadPartCopy to consolidate all the chunks into a large segment object. The path is baseDir/segment. After the copy is successful, we can asynchronously delete all baseDir/chunk-*, and update S3SegmentMetadata to:\n0 -> \n\nRestart - Consolidated\n\nAt this point, we first concatenate the path baseDir/segment to determine whether the object exists. If it exists, it is a consolidated large Segment object, so we record the corresponding metadata locally. read0() can directly read the object based on the offset. Then, we check whether the baseDir/chunk-* objects currently exist. If they do, it means that the asynchronous deletion has not been successful, so we can try to delete them again asynchronously.\n\nRestart - Not Consolidated\n\nIf the concatenated path baseDir/segment does not exist, it may be due to consolidation failure or the current Segment not being fully written. We can list all the chunk-* paths under baseDir and then determine whether they are full (an interface can be added to determine this during recovery). If it is full, we can consolidate and delete it asynchronously. If it is not full, we can restore the metadata S3SegmentMetadata normally.\nPossible Optimizations\nUpload Buffer Pooling\nA general UploadBufferPool can be used as the upload buffer. Each time commit0 is called, the data is first put into the corresponding Buffer in the pool. When the overall buffer pool reaches the set threshold, the actual data upload is triggered.\n\n\nWhen commit0 is called, the data is read into the corresponding Buffer in the queue.\nWhen the Upload Buffer Pool reaches the threshold, the actual data is uploaded to S3. All the data in each queue's Buffer forms an object.\nUpdate S3SegmentMetadata and TieredFileSegment#commitPosition.\n\nTasks\n\nImplement and run the basic solution.\nTest the performance of the basic solution.\nIf performance is related to uploading in batches, implement an optimization solution with uploading buffering pooling and compare its performance.\n\nElimination implement\nnew configuration\n\n\n\nConfiguration\nType\nUnit\nDefault Value\nFunction\n\n\n\n\ns3Region\nString\n\n\nRegion name about S3 service\n\n\ns3Bucket\nString\n\n\nobject's store bucket in S3\n\n\ns3AccessKey\nString\n\n\nIAM's accessKey\n\n\ns3SecretAccessKey\nString\n\n\nIAM's Secret AccessKey\n\n\ns3ChunkSize\nlong\nbytes\n4 *1024* 1024\nchunk num in one S3Segment\n\n\nreadaheadChunkNum\nint\n\n8\nreadahead chunk num in each read0 calling\n\n\ns3ClientThreadNum\nint\n\ntieredStoreGroupCommitSize/s3ChunkSize\nthreads' num in S3Client's threadpool\n\n\n\nA segment is treated as a logical file and is divided into multiple physical files, or multiple physical objects, in the S3 view. We assume that each physical object has a default size of 4 MB, which is named chunk.\nFor ease of process representation, we assume that readaheadChunkNum is 2 in the following.\nProcess\n\nCreate S3SegmentFile object\u3002\n\nThis is done in the S3Segment#S3Segment constructor. The path of the logical segment file is constructed by concatenating the following components according to a set of rules: clusterName/brokerName/topic/queue/type-baseOffset. The path below this point is referred to as baseDir.\n\nCreate real logical segment\n\nThat is, the S3Segment#createFile() method is called. Since no data has been written yet, we need to create the first chunk object and allocate 4MB of memory to cache the data for this chunk. We request the creation of an object from S3 in the format baseDir/chunk-startOffset, which means creating a baseDir/chunk-0 object in S3 now.\n\ncommit\n\nThe Segment#commit0() method is called.\nWe assume that wrting 2MB data this time.\n\nThe data is directlly writed into chunk-0, and uploaded to S3.\n\nread\n\nThat is, the S3Segment#read0() method is called. Suppose we are currently reading 1MB of data with position = 0 and length = 1024. Then it directly hits in the local chunk-0 buffer and returns.\n\ncommit\n\nSuppose this time we write position= 2048, length= 12 * 1024 data, that is, submit 12MB of data from 2MB position.\n\nAt this point, the first 2MB of chunk-0 is cached locally, so we can directly concatenate the first 2MB of chunk-0 with the first 2MB of the stream to form a complete chunk-0. Next, we correctly locate the first 2MB of chunk-4096, chunk-8192, and chunk-12288, and then upload them to S3. For the case of multiple chunks uploading at the same time, we use asynchronous/thread pool to upload them. If some chunks fail to upload, they are cached and then retried in the background asynchronously. If they fail multiple times, appropriate logical processing is performed.\n\nread\n\nAfter the above commit, only the first 2MB of chunk-12288 is cached locally. Now, we read 4096 bytes of data starting from position = 2048, which means reading the second half of chunk-0 and the first half of chunk-4096. Since we have enabled the pre-reading mechanism and the parameter is 2, we need to read two more chunks. Considering that we only read half of chunk-4096, we only need to read one more chunk, which is chunk-8192.\n\nThen we read chunk-0, chunk-4096, and chunk-8192 from S3. According to the pre-reading mechanism, we do not save chunk-0 and only save chunk-4096 and chunk-8192 in memory.\n\n\nread\n\nNow, we read 4096 bytes of data starting from position = 6144, which means reading the second half of chunk-4096 and the first half of chunk-8192. Since we have pre-loaded chunk-4096 and chunk-8192 into memory, we can directly return the data without reading from S3.\n\nSegment is full\n\nAt this point, we can asynchronously trigger an uploadPartCopy operation to consolidate all the chunks into a single large segment object, and record the basic information of the segment in the object metadata. The object path is clusterName/brokerName/topic/queue/type-baseOffset-seg. After the copy operation is successful, we can asynchronously delete the parent path of the chunks.\n\nrestart with segment\n\nNow we concatenate the path clusterName/brokerName/topic/queue/type-baseOffset-seg and check whether the object exists. If it exists, it means it is the already organized large Segment object, then we record the corresponding metadata locally, and read0() can directly read the object based on the offset. Next, we check if there is an object under .../type-baseOffset. If it exists, it means the asynchronous deletion has not been successful, so we can re-attempt asynchronous deletion.\n\nrestart with unexist segment\n\nIf the path .../type-baseOffset-seg does not exist, it may be due to failed consolidation or the current segment has not been written to capacity. In this case, we can list all the chunk files under the path and then determine if the segment has been fully written (this can be checked by adding an interface that is called during recovery). If the segment has been fully written, we can consolidate the chunks asynchronously and then delete them. If the segment has not been fully written, we can simply recover the latest chunk by caching it.\nAdvantages and disadvantages\n\nAdvantages\n\n\nThe pre-fetching mechanism and the caching of incomplete chunks can help reduce network requests.\nOptimal use case for this design is sequential read access, which fully utilizes the prefetching mechanism.\n\n\nDisadvantages\n\n\nchunk caches can lead to excessive memory usage. Suppose that 1000 queues, even if only one chunk is cached for one queue, can reach 4GB of memory usage.\n"
}